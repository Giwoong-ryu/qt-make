"""
Groq Whisper ìŒì„± ì¸ì‹ ì„œë¹„ìŠ¤
"""
import logging
import os

from groq import Groq

from app.config import get_settings

logger = logging.getLogger(__name__)
settings = get_settings()


class WhisperService:
    """Groq Whisper Large v3 Turbo STT ì„œë¹„ìŠ¤"""

    # ìë§‰ ê¸¸ì´ ì„¤ì • í”„ë¦¬ì…‹
    SUBTITLE_PRESETS = {
        "short": {  # QT ì˜ìƒ ìµœì í™” (ê¸°ë³¸ê°’)
            "max_chars_per_line": 8,
            "max_chars_per_subtitle": 16,
        },
        "long": {  # Netflix í•œêµ­ì–´ ê¸°ì¤€
            "max_chars_per_line": 16,
            "max_chars_per_subtitle": 32,
        }
    }

    def __init__(self, subtitle_length: str = "short"):
        self.client = Groq(api_key=settings.GROQ_API_KEY)
        self.model = "whisper-large-v3-turbo"  # $0.04/hour

        # ìë§‰ ê¸¸ì´ ì„¤ì • ì ìš©
        preset = self.SUBTITLE_PRESETS.get(subtitle_length, self.SUBTITLE_PRESETS["short"])
        self.MAX_CHARS_PER_LINE = preset["max_chars_per_line"]
        self.MAX_CHARS_PER_SUBTITLE = preset["max_chars_per_subtitle"]
        self.subtitle_length = subtitle_length

        logger.info(f"WhisperService initialized with subtitle_length={subtitle_length} "
                    f"(MAX_CHARS_PER_LINE={self.MAX_CHARS_PER_LINE}, "
                    f"MAX_CHARS_PER_SUBTITLE={self.MAX_CHARS_PER_SUBTITLE})")
    MIN_DURATION = 0.83  # ìë§‰ ìµœì†Œ í‘œì‹œ ì‹œê°„ (ì´ˆ) - Netflix ê¸°ì¤€ 5/6ì´ˆ
    MAX_DURATION = 5.0  # ìë§‰ ìµœëŒ€ í‘œì‹œ ì‹œê°„ (ì´ˆ) - ë” ì§§ê²Œ (7ì´ˆâ†’5ì´ˆ)
    MIN_GAP_FRAMES = 2  # ìë§‰ ê°„ ìµœì†Œ ê°„ê²© (í”„ë ˆì„) @ 30fps = ì•½ 67ms
    TARGET_CPS = 14  # ëª©í‘œ CPS (Characters Per Second) - ë” ì—¬ìœ ë¡­ê²Œ (16â†’14)

    # í•œêµ­ì–´ ë¬¸ì¥ ì¢…ê²° íŒ¨í„´ (ìš°ì„ ìˆœìœ„ ìˆœ)
    # Netflix/Kss ê¸°ì¤€: í™•ì‹¤í•œ ì¢…ê²°ì–´ë¯¸ë§Œ í¬í•¨
    # ì£¼ì˜: 1ê¸€ì ì¢…ê²°ì–´ë¯¸(ê°€, ì™€, ë´, ì¤˜)ëŠ” ì¡°ì‚¬ì™€ í˜¼ë™ë˜ë¯€ë¡œ ì œì™¸!
    KOREAN_SENTENCE_ENDINGS = (
        # ê²©ì‹ì²´ ì¢…ê²°ì–´ë¯¸ (ê°€ì¥ í™•ì‹¤í•¨)
        'ìŠµë‹ˆë‹¤', 'ì…ë‹ˆë‹¤', 'ë©ë‹ˆë‹¤', 'í•©ë‹ˆë‹¤', 'ê°‘ë‹ˆë‹¤', 'ì˜µë‹ˆë‹¤',
        'ì‹­ì‹œì˜¤', 'ì‹­ë‹ˆë‹¤', 'ì…¨ìŠµë‹ˆë‹¤', 'ì—ˆìŠµë‹ˆë‹¤', 'ì•˜ìŠµë‹ˆë‹¤',
        'ê² ìŠµë‹ˆë‹¤', 'ì´ì—ˆìŠµë‹ˆë‹¤', 'ì…¨ì–´ìš”', 'ì˜€ìŠµë‹ˆë‹¤',
        # ì¡´ëŒ“ë§ ì¢…ê²°ì–´ë¯¸
        'ì„¸ìš”', 'ì‹œì£ ', 'ì‹œë„¤ìš”', 'ìœ¼ì„¸ìš”', 'ìœ¼ì‹œì£ ',
        'ìœ¼ì‹­ì‹œì˜¤', 'í•˜ì„¸ìš”', 'ì£¼ì„¸ìš”', 'ë³´ì„¸ìš”', 'ê°€ì„¸ìš”', 'ì˜¤ì„¸ìš”',
        # í•´ìš”ì²´ ì¢…ê²°ì–´ë¯¸
        'í•´ìš”', 'ë„¤ìš”', 'ì£ ', 'ì–´ìš”', 'ì•„ìš”', 'ì˜ˆìš”', 'ì—ìš”',
        'ê² ì£ ', 'ì–ì•„ìš”', 'ê±°ì˜ˆìš”', 'ê±´ê°€ìš”', 'ë‚˜ìš”', 'ì„ê¹Œìš”',
        'í–ˆì–´ìš”', 'ëì–´ìš”', 'ì˜€ì–´ìš”', 'ì´ì—ìš”', 'ë°ìš”', 'ë˜ìš”',
        'ë³¼ê²Œìš”', 'í• ê²Œìš”', 'ì¤„ê²Œìš”', 'ê°ˆê²Œìš”', 'ì˜¬ê²Œìš”',
        'ë´ìš”', 'í•´ë´ìš”', 'ê°€ìš”', 'ì™€ìš”', 'ì¤˜ìš”',
        # ë°˜ë§ ì¢…ê²°ì–´ë¯¸ (2ê¸€ì ì´ìƒë§Œ! 1ê¸€ìëŠ” ì¡°ì‚¬ì™€ í˜¼ë™)
        'í–ˆë‹¤', 'ëë‹¤', 'ì—ˆë‹¤', 'ì•˜ë‹¤', 'ì¸ë‹¤', 'í•œë‹¤', 'ì´ë‹¤',
        'ê°„ë‹¤', 'ì˜¨ë‹¤', 'ë³¸ë‹¤', 'ë˜ë‹¤', 'ì¤€ë‹¤', 'ì“´ë‹¤',
        'í–ˆì–´', 'ëì–´', 'ì—ˆì–´', 'ì•˜ì–´', 'ê±°ì•¼', 'ì´ì•¼',
        'í•´ë´', 'ê°€ë´', 'ì™€ë´',  # 2ê¸€ì ì´ìƒ ë°˜ë§
        'í•´ì¤˜', 'ê°€ì¤˜', 'ì™€ì¤˜',  # 2ê¸€ì ì´ìƒ ë°˜ë§
        # ì˜ë¬¸í˜• ì¢…ê²°ì–´ë¯¸
        'ìŠµë‹ˆê¹Œ', 'ì…ë‹ˆê¹Œ', 'ëŠ”ê°€', 'ì€ê°€', 'ëƒ', 'ë‚˜',
        'ì„ê¹Œ', 'í• ê¹Œ', 'ë³¼ê¹Œ', 'ê°ˆê¹Œ',
        'í• ë˜', 'ë³¼ë˜', 'ê°ˆë˜', 'í• ê¹Œìš”', 'ë³¼ê¹Œìš”',
        # ì²­ìœ í˜•/ëª…ë ¹í˜•
        'ìì‹œë‹¤', 'í•©ì‹œë‹¤', 'í•˜ì', 'í•´ë¼', 'ê°€ì', 'ë³´ì',
        'í•˜ì„¸ìš”', 'ê°€ì„¸ìš”', 'ì˜¤ì„¸ìš”', 'ë³´ì„¸ìš”',
        # ê°íƒ„í˜•
        'êµ¬ë‚˜', 'êµ°ìš”', 'êµ¬ë§Œ', 'ë”ë¼', 'ë˜ë°ìš”',
    )

    # í•œêµ­ì–´ ì¡°ì‚¬ (ì ˆëŒ€ ëŠìœ¼ë©´ ì•ˆ ë˜ëŠ” íŒ¨í„´!)
    # ì´ íŒ¨í„´ìœ¼ë¡œ ëë‚˜ë©´ ë¬¸ì¥ì´ ë¶ˆì™„ì „í•¨
    KOREAN_PARTICLES = (
        # ì£¼ê²©/ëª©ì ê²© ì¡°ì‚¬
        'ì´', 'ê°€', 'ì„', 'ë¥¼', 'ì€', 'ëŠ”',
        # ë¶€ì‚¬ê²© ì¡°ì‚¬
        'ì—', 'ì—ì„œ', 'ì—ê²Œ', 'í•œí…Œ', 'ê»˜',
        'ë¡œ', 'ìœ¼ë¡œ', 'ë¶€í„°', 'ê¹Œì§€', 'ë³´ë‹¤',
        'ì™€', 'ê³¼', 'ë‘', 'ì´ë‘', 'í•˜ê³ ',
        # ê´€í˜•ê²©/ì†Œìœ ê²©
        'ì˜',
        # ë³´ì¡°ì‚¬
        'ë„', 'ë§Œ', 'ë¿', 'ë°–ì—', 'ì¡°ì°¨', 'ë§ˆì €', 'ê¹Œì§€ë„',
        # ì ‘ì†ì¡°ì‚¬
        'ë‚˜', 'ì´ë‚˜', 'ê±°ë‚˜', 'ë“ ì§€', 'ë“ ê°€',
    )

    # í•œêµ­ì–´ ì ‘ì†ë¶€ì‚¬ (ìƒˆ ë¬¸ì¥/ì ˆ ì‹œì‘ ì‹ í˜¸)
    # ì´ ë‹¨ì–´ë“¤ì´ ë‚˜ì˜¤ë©´ ì•ì—ì„œ ëŠì–´ì•¼ í•¨
    # ì°¸ì¡°: docs/KOREAN_SUBTITLE_SEGMENTATION_GUIDE.md (Netflix/Kss ê¸°ì¤€)
    KOREAN_CONJUNCTIONS = (
        # ìˆœì ‘ (6ê°œ)
        'ê·¸ë¦¬ê³ ', 'ê·¸ë˜ì„œ', 'ê·¸ëŸ¬ë¯€ë¡œ', 'ë”°ë¼ì„œ', 'ê·¸ëŸ¬ë‹ˆê¹Œ', 'ê·¸ëŸ¬ë‹ˆ',
        # ì—­ì ‘ (6ê°œ)
        'ê·¸ëŸ¬ë‚˜', 'í•˜ì§€ë§Œ', 'ê·¸ë ‡ì§€ë§Œ', 'ê·¸ëŸ°ë°', 'ë‹¤ë§Œ', 'ë°˜ë©´ì—',
        # ì „í™˜ (5ê°œ)
        'ê·¸ë˜ë„', 'ì–´ì¨Œë“ ', 'ì•„ë¬´íŠ¼', 'í•œí¸', 'ê·¸ë‚˜ì €ë‚˜',
        # ì˜ˆì‹œ/ë¶€ì—° (5ê°œ)
        'ì˜ˆë¥¼ ë“¤ì–´', 'ì¦‰', 'ë‹¤ì‹œ ë§í•´', 'íŠ¹íˆ', 'ë¬¼ë¡ ',
        # ì¶”ê°€ (ì„¤êµ/êµìœ¡ ì½˜í…ì¸  íŠ¹í™”)
        'ë˜í•œ', 'ê²Œë‹¤ê°€', 'ë”êµ¬ë‚˜', 'ë¿ë§Œì•„ë‹ˆë¼', 'ì‹¬ì§€ì–´',
        'ì™œëƒí•˜ë©´', 'ê·¸ëŸ¬ë©´', 'ê·¸ë ‡ë‹¤ë©´', 'ë§Œì•½', 'ë§Œì¼', 'ì˜¤íˆë ¤',
    )

    # ë³´ì¡° ìš©ì–¸ íŒ¨í„´ (ë¶„ë¦¬í•˜ë©´ ì•ˆ ë˜ëŠ” ë³µí•© í‘œí˜„)
    # "í•  ìˆ˜" ê°™ì€ ì˜ì¡´ëª…ì‚¬ + ë³´ì¡°ë™ì‚¬ êµ¬ì„±
    AUXILIARY_VERB_PATTERNS = (
        'ìˆ˜ ìˆ',   # "í•  ìˆ˜ ìˆë‹¤", "ë³¼ ìˆ˜ ìˆì–´"
        'ìˆ˜ ì—†',   # "í•  ìˆ˜ ì—†ë‹¤", "ë³¼ ìˆ˜ ì—†ì–´"
        'ì¤„ ì•Œ',   # "í•  ì¤„ ì•Œì•„", "í•  ì¤„ ì•Œì•˜ì–´"
        'ì¤„ ëª¨',   # "í•  ì¤„ ëª¨ë¥´ë‹¤"
        'ê²Œ ë˜',   # "í•˜ê²Œ ë˜ë‹¤", "ë³´ê²Œ ëì–´"
        'ê²Œ í•˜',   # "í•˜ê²Œ í•˜ë‹¤", "ë³´ê²Œ í•´"
        'ì§€ ì•Š',   # "í•˜ì§€ ì•Šë‹¤", "ê°€ì§€ ì•Šì•„"
        'ì§€ ë§',   # "í•˜ì§€ ë§ì•„", "ê°€ì§€ ë§ˆ"
        'ê³  ì‹¶',   # "í•˜ê³  ì‹¶ë‹¤", "ë³´ê³  ì‹¶ì–´"
        'ê³  ìˆ',   # "í•˜ê³  ìˆë‹¤", "ë³´ê³  ìˆì–´"
    )

    def get_transcription(
        self,
        audio_path: str,
        language: str = "ko",
        initial_prompt: str | None = None
    ):
        """
        Whisper API í˜¸ì¶œ â†’ raw transcription ê°ì²´ ë°˜í™˜
        (êµì •ì„ ë¨¼ì € ì ìš©í•˜ê¸° ìœ„í•´ SRT ìƒì„±ê³¼ ë¶„ë¦¬)

        Args:
            audio_path: ì˜¤ë””ì˜¤ íŒŒì¼ ê²½ë¡œ
            language: ì–¸ì–´ ì½”ë“œ (ko, en, etc)
            initial_prompt: Whisper íŒíŠ¸

        Returns:
            transcription: Groq Whisper API ì‘ë‹µ ê°ì²´ (verbose_json)
        """
        try:
            logger.info(f"Transcribing audio: {audio_path}")

            # ê¸°ë³¸ í”„ë¡¬í”„íŠ¸ (êµíšŒ/ì„¤êµ ê´€ë ¨ ìš©ì–´ + ì„±ê²½ ë¹„ìœ )
            default_prompt = (
                "ë¬µìƒ, ë§ì”€, ì€í˜œ, ì„±ê²½, í•˜ë‚˜ë‹˜, ì˜ˆìˆ˜ë‹˜, ì˜ˆìˆ˜ ê·¸ë¦¬ìŠ¤ë„, ì„±ë ¹, ê¸°ë„, ì°¬ì–‘, ì˜ˆë°°, êµ¬ì›, ì‹­ìê°€, ë¶€í™œ, "
                "ê²¨ìì”¨, ì”¨ì•—, ì”¨ë¿Œë¦¬ëŠ” ì, í¬ë„ì›, ë¬´í™”ê³¼ë‚˜ë¬´, ê°ëŒì‚°, ì˜¬ë¦¬ë¸Œ, ë¹›ê³¼ ì†Œê¸ˆ, ì–‘ê³¼ ëª©ì, ì²œêµ­, ë¹„ìœ , "
                "ë§ˆê°€ë³µìŒ, ë§ˆíƒœë³µìŒ, ëˆ„ê°€ë³µìŒ, ìš”í•œë³µìŒ, ì‚¬ë„í–‰ì „, ë¡œë§ˆì„œ, ê³ ë¦°ë„ì „ì„œ, ì—ë² ì†Œì„œ, ë¹Œë¦½ë³´ì„œ, "
                "ì•„ë©˜, í• ë ë£¨ì•¼, í˜¸ì‚°ë‚˜, ë§ˆë¼ë‚˜íƒ€, ì£¼ë‹˜, ê·¸ë¦¬ìŠ¤ë„, ë©”ì‹œì•„, êµ¬ì„¸ì£¼, ë³µìŒ, ì œì, ì‚¬ë„, "
                "ê½ƒë™ì‚°, êµíšŒ, ì„±ë„, í˜•ì œ, ìë§¤, ëª©ì‚¬ë‹˜, ì§‘ì‚¬ë‹˜, ê¶Œì‚¬ë‹˜, ì¥ë¡œë‹˜"
            )
            prompt = initial_prompt if initial_prompt else default_prompt

            # ì˜¤ë””ì˜¤ íŒŒì¼ ì½ê¸°
            with open(audio_path, "rb") as audio_file:
                # Groq Whisper API í˜¸ì¶œ (word ë‹¨ìœ„ íƒ€ì„ìŠ¤íƒ¬í”„)
                transcription = self.client.audio.transcriptions.create(
                    file=audio_file,
                    model=self.model,
                    language=language,  # í•œêµ­ì–´ ìµœì í™”
                    response_format="verbose_json",  # íƒ€ì„ìŠ¤íƒ¬í”„ í¬í•¨
                    timestamp_granularities=["word"],  # ë‹¨ì–´ ë‹¨ìœ„
                    temperature=0.0,  # ì¼ê´€ì„± ìµœëŒ€í™”
                    prompt=prompt  # ë„ë©”ì¸ íŠ¹í™” íŒíŠ¸
                )

            return transcription

        except Exception as e:
            logger.exception(f"Transcription failed: {e}")
            raise

    def create_srt_from_transcription(
        self,
        transcription,
        audio_path: str
    ) -> str:
        """
        êµì •ëœ transcription ê°ì²´ â†’ SRT íŒŒì¼ ìƒì„±

        Args:
            transcription: Whisper API ì‘ë‹µ (êµì • í›„)
            audio_path: ì˜¤ë””ì˜¤ íŒŒì¼ ê²½ë¡œ (SRT ì €ì¥ ê²½ë¡œ ê²°ì •ìš©)

        Returns:
            srt_path: ìƒì„±ëœ SRT íŒŒì¼ ê²½ë¡œ
        """
        try:
            # SRT í˜•ì‹ìœ¼ë¡œ ë³€í™˜ (ì ì ˆí•œ ê¸¸ì´ë¡œ ë¶„í• )
            srt_content = self._convert_to_srt(transcription)

            # SRT íŒŒì¼ ì €ì¥ (í™•ì¥ì ê´€ê³„ì—†ì´ .srtë¡œ ë³€í™˜)
            base_path = os.path.splitext(audio_path)[0]
            srt_path = f"{base_path}.srt"
            with open(srt_path, "w", encoding="utf-8") as srt_file:
                srt_file.write(srt_content)

            logger.info(f"SRT file created: {srt_path}")
            return srt_path

        except Exception as e:
            logger.exception(f"SRT creation failed: {e}")
            raise

    def transcribe_to_srt(
        self,
        audio_path: str,
        language: str = "ko",
        initial_prompt: str | None = None
    ) -> str:
        """
        MP3 â†’ SRT ìë§‰ íŒŒì¼ ìƒì„± (ê¸°ì¡´ í˜¸í™˜ì„± ìœ ì§€)

        Args:
            audio_path: MP3 íŒŒì¼ ê²½ë¡œ
            language: ì–¸ì–´ ì½”ë“œ (ko, en, etc)
            initial_prompt: Whisper íŒíŠ¸ (êµíšŒë³„ ìš©ì–´, ìµœëŒ€ 224í† í°)

        Returns:
            srt_path: ìƒì„±ëœ SRT íŒŒì¼ ê²½ë¡œ
        """
        transcription = self.get_transcription(audio_path, language, initial_prompt)
        return self.create_srt_from_transcription(transcription, audio_path)

    def _convert_to_srt(self, transcription) -> str:
        """
        Groq verbose_json â†’ SRT í˜•ì‹ ë³€í™˜ (ì ì ˆí•œ ê¸¸ì´ë¡œ ë¶„í• )

        - ìµœëŒ€ 40ì (2ì¤„ x 20ì)
        - ìµœëŒ€ 5ì´ˆ í‘œì‹œ
        - ìì—°ìŠ¤ëŸ¬ìš´ ëŠê¹€ (í•œêµ­ì–´ ë¬¸ì¥ ì¢…ê²° íŒ¨í„´ ê¸°ì¤€)
        - ìë§‰ ê°„ ê²¹ì¹¨ ë°©ì§€
        """
        srt_lines = []
        index = 1

        # words ì†ì„±ì´ ìˆìœ¼ë©´ word ë‹¨ìœ„, ì—†ìœ¼ë©´ segment ë‹¨ìœ„
        if hasattr(transcription, 'words') and transcription.words:
            subtitles = self._group_words_into_subtitles(transcription.words)
        else:
            # fallback: segment ê¸°ë°˜
            subtitles = self._split_segments_into_subtitles(transcription.segments)

        # ë¶ˆì™„ì „í•œ ìë§‰ ë³‘í•© (ì¡°ì‚¬ë¡œ ëë‚˜ëŠ” ì§§ì€ ìë§‰ í•©ì¹˜ê¸°)
        subtitles = self._merge_incomplete_subtitles(subtitles)

        # ìë§‰ ê²¹ì¹¨ ë°©ì§€: ë‹¤ìŒ ìë§‰ ì‹œì‘ ì „ì— í˜„ì¬ ìë§‰ ì¢…ë£Œ
        subtitles = self._prevent_subtitle_overlap(subtitles)

        for subtitle in subtitles:
            start_time = self._format_timestamp(subtitle['start'])
            end_time = self._format_timestamp(subtitle['end'])
            text = subtitle['text']

            # í•­ìƒ 2ì¤„ë¡œ ë¶„ë¦¬ (QT ì˜ìƒ í†µì¼ì„±)
            text = self._split_into_two_lines(text)

            srt_lines.append(f"{index}")
            srt_lines.append(f"{start_time} --> {end_time}")
            srt_lines.append(text)
            srt_lines.append("")

            index += 1

        return "\n".join(srt_lines)

    def _merge_incomplete_subtitles(self, subtitles: list) -> list:
        """
        ë¶ˆì™„ì „í•œ ìë§‰ ë³‘í•© (í•œêµ­ì–´ ë¬¸ë²• í›„ì²˜ë¦¬)

        ë³‘í•© ì¡°ê±´:
        1. í˜„ì¬ ìë§‰ì´ ì¡°ì‚¬ë¡œ ëë‚¨ (ë¬¸ì¥ ë¶ˆì™„ì „)
        2. í˜„ì¬ ìë§‰ì´ ë„ˆë¬´ ì§§ìŒ (3ê¸€ì ì´í•˜)
        3. ë³‘í•© í›„ì—ë„ ìµœëŒ€ ê¸¸ì´/ì‹œê°„ ì´ˆê³¼í•˜ì§€ ì•ŠìŒ

        ì˜ˆì‹œ:
        - "ì˜¤ëŠ˜ ìš°ë¦¬ê°€" + "ì˜ˆë°°ë“œë¦½ë‹ˆë‹¤" â†’ "ì˜¤ëŠ˜ ìš°ë¦¬ê°€ ì˜ˆë°°ë“œë¦½ë‹ˆë‹¤"
        - "ì”¨ë³´ë‹¤ ì‘ì€ ê²ƒì´ë¡œ" + "ë¼ ì‹¬ê¸´ í›„ì—ëŠ”" â†’ "ì”¨ë³´ë‹¤ ì‘ì€ ê²ƒì´ë¡œ ë¼ ì‹¬ê¸´ í›„ì—ëŠ”"
        """
        if not subtitles or len(subtitles) < 2:
            return subtitles

        merged = []
        i = 0

        while i < len(subtitles):
            current = subtitles[i].copy()

            # ë§ˆì§€ë§‰ ìë§‰ì´ë©´ ê·¸ëƒ¥ ì¶”ê°€
            if i >= len(subtitles) - 1:
                merged.append(current)
                break

            # í˜„ì¬ ìë§‰ ë¶„ì„
            current_text = current['text'].strip()
            words = current_text.split()
            last_word = words[-1] if words else ""

            # ë³‘í•© í•„ìš” ì—¬ë¶€ íŒë‹¨
            should_merge = False

            # ì¡°ê±´ 1: ì¡°ì‚¬ë¡œ ëë‚˜ë©´ ë¶ˆì™„ì „
            for particle in self.KOREAN_PARTICLES:
                if last_word.endswith(particle) and len(last_word) <= len(particle) + 3:
                    should_merge = True
                    break

            # ì¡°ê±´ 2: ë„ˆë¬´ ì§§ì€ ìë§‰ (5ê¸€ì ì´í•˜)
            if len(current_text.replace(' ', '')) <= 5:
                should_merge = True

            # ì¡°ê±´ 3: ì—°ê²°ì–´ë¯¸ë¡œ ëë‚˜ë©´ ë¶ˆì™„ì „
            for connecting in self.KOREAN_CONNECTING_ENDINGS:
                if current_text.endswith(connecting):
                    should_merge = True
                    break

            if should_merge:
                next_sub = subtitles[i + 1]
                merged_text = current_text + " " + next_sub['text'].strip()
                merged_duration = next_sub['end'] - current['start']

                # ë³‘í•© ê°€ëŠ¥ ì¡°ê±´: ê¸¸ì´/ì‹œê°„ ì´ˆê³¼ ì•ˆ í•¨
                if (len(merged_text) <= self.MAX_CHARS_PER_SUBTITLE * 1.5 and
                    merged_duration <= self.MAX_DURATION * 1.2):
                    # ë³‘í•© ì‹¤í–‰
                    current['text'] = merged_text
                    current['end'] = next_sub['end']
                    merged.append(current)
                    i += 2  # ë‹¤ìŒ ìë§‰ ìŠ¤í‚µ
                    continue

            # ë³‘í•© ì•ˆ í•˜ê³  ê·¸ëŒ€ë¡œ ì¶”ê°€
            merged.append(current)
            i += 1

        return merged

    def _prevent_subtitle_overlap(self, subtitles: list) -> list:
        """
        ìë§‰ ê°„ ê²¹ì¹¨ ë°©ì§€ + ìµœì†Œ/ìµœëŒ€ í‘œì‹œ ì‹œê°„ ì ìš©

        Netflix/YouTube í‘œì¤€:
        - ìë§‰ ê°„ ìµœì†Œ 2í”„ë ˆì„(67ms @ 30fps) ê°„ê²©
        - ìµœì†Œ í‘œì‹œ ì‹œê°„: 1.5ì´ˆ (ê¹œë¹¡ì„ ë°©ì§€)
        - ìµœëŒ€ í‘œì‹œ ì‹œê°„: 7ì´ˆ
        """
        if not subtitles:
            return subtitles

        MIN_GAP = self.MIN_GAP_FRAMES / 30.0  # 2í”„ë ˆì„ @ 30fps = ì•½ 67ms

        for i in range(len(subtitles)):
            # 1. ìµœì†Œ í‘œì‹œ ì‹œê°„ ë³´ì¥ (ê¹œë¹¡ì„ ë°©ì§€)
            duration = subtitles[i]['end'] - subtitles[i]['start']
            if duration < self.MIN_DURATION:
                subtitles[i]['end'] = subtitles[i]['start'] + self.MIN_DURATION

            # 2. ìµœëŒ€ í‘œì‹œ ì‹œê°„ ì œí•œ
            duration = subtitles[i]['end'] - subtitles[i]['start']
            if duration > self.MAX_DURATION:
                subtitles[i]['end'] = subtitles[i]['start'] + self.MAX_DURATION

            # 3. ë‹¤ìŒ ìë§‰ê³¼ ê²¹ì¹¨ ë°©ì§€
            if i < len(subtitles) - 1:
                next_start = subtitles[i + 1]['start']

                # ê²¹ì¹˜ê±°ë‚˜ ê°„ê²©ì´ ë¶€ì¡±í•˜ë©´ í˜„ì¬ ìë§‰ ì¢…ë£Œ ì‹œê°„ ì¡°ì •
                if subtitles[i]['end'] >= next_start - MIN_GAP:
                    # ë‹¤ìŒ ìë§‰ ì‹œì‘ ì „ì— ì¢…ë£Œ (ìµœì†Œ ê°„ê²© ìœ ì§€)
                    subtitles[i]['end'] = next_start - MIN_GAP

                    # ì¢…ë£Œ ì‹œê°„ì´ ì‹œì‘ ì‹œê°„ë³´ë‹¤ ë¹ ë¥´ë©´ ìµœì†Œ 0.5ì´ˆ ìœ ì§€
                    if subtitles[i]['end'] <= subtitles[i]['start']:
                        subtitles[i]['end'] = subtitles[i]['start'] + 0.5

        return subtitles

    def _group_words_into_subtitles(self, words: list) -> list:
        """
        ë‹¨ì–´ë“¤ì„ ì ì ˆí•œ ê¸¸ì´ì˜ ìë§‰ ë¸”ë¡ìœ¼ë¡œ ê·¸ë£¹í™”

        Netflix/Kss ê¸°ì¤€ ë¶„ë¦¬ ê·œì¹™:
        1. ì ‘ì†ë¶€ì‚¬ ì•ì—ì„œ ëŠê¸° (í•˜ì§€ë§Œ, ê·¸ëŸ¬ë‚˜, ê·¸ë˜ì„œ ë“±)
        2. ì¢…ê²°ì–´ë¯¸ ë’¤ì—ì„œ ëŠê¸° (ìŠµë‹ˆë‹¤, í•´ìš” ë“±)
        3. ê¸€ììˆ˜/ì‹œê°„ ì´ˆê³¼ ì‹œ ëŠê¸°
        4. ì—°ê²°ì–´ë¯¸ëŠ” ëŠì§€ ì•Šê¸° (í•´ì„œ, ë‹ˆê¹Œ, ì§€ë§Œ ë“±)
        """
        subtitles = []
        current_words = []
        current_text = ""
        current_start = None

        for word_data in words:
            word = word_data.get('word', '').strip()
            start = word_data.get('start', 0)
            end = word_data.get('end', 0)

            if not word:
                continue

            # ì ‘ì†ë¶€ì‚¬ ì•ì—ì„œ ëŠê¸° ì²´í¬ (Netflix ìŠ¤íƒ€ì¼)
            # "ì•‰ìœ¼ì‹­ë‹ˆë‹¤ í•˜ì§€ë§Œ" â†’ "ì•‰ìœ¼ì‹­ë‹ˆë‹¤" / "í•˜ì§€ë§Œ..."
            if current_text and self._should_break_before_word(word):
                # í˜„ì¬ ë¸”ë¡ ì €ì¥ (ì ‘ì†ë¶€ì‚¬ ì „ê¹Œì§€)
                if current_words:
                    subtitles.append({
                        'start': current_start,
                        'end': current_words[-1].get('end', end),
                        'text': current_text
                    })
                # ì ‘ì†ë¶€ì‚¬ë¶€í„° ìƒˆ ë¸”ë¡ ì‹œì‘
                current_words = [word_data]
                current_text = word
                current_start = start
                continue

            # ì²« ë‹¨ì–´
            if current_start is None:
                current_start = start

            # ìƒˆ ë‹¨ì–´ ì¶”ê°€ ì‹œ ê¸¸ì´/ì‹œê°„ ì²´í¬
            new_text = (current_text + " " + word).strip() if current_text else word
            duration = end - current_start

            # í•œêµ­ì–´ ë¬¸ì¥ ì¢…ê²° íŒ¨í„´ ì²´í¬
            is_sentence_end = self._is_korean_sentence_end(new_text)

            # ì¡°ê±´: ê¸€ììˆ˜ ì´ˆê³¼ ë˜ëŠ” ì‹œê°„ ì´ˆê³¼ ë˜ëŠ” ë¬¸ì¥ ë
            should_break = (
                len(new_text) > self.MAX_CHARS_PER_SUBTITLE or
                duration > self.MAX_DURATION or
                is_sentence_end
            )

            if should_break and current_text:
                # ë¬¸ì¥ ëì´ë©´ í˜„ì¬ ë‹¨ì–´ê¹Œì§€ í¬í•¨í•´ì„œ ì €ì¥
                if is_sentence_end and len(new_text) <= self.MAX_CHARS_PER_SUBTITLE:
                    current_words.append(word_data)
                    current_text = new_text

                # í˜„ì¬ ë¸”ë¡ ì €ì¥
                subtitles.append({
                    'start': current_start,
                    'end': current_words[-1].get('end', end),
                    'text': current_text
                })

                # ìƒˆ ë¸”ë¡ ì‹œì‘
                if is_sentence_end and len(new_text) <= self.MAX_CHARS_PER_SUBTITLE:
                    # ë¬¸ì¥ ëìœ¼ë¡œ ëŠì—ˆìœ¼ë©´ ìƒˆ ë¸”ë¡ì€ ë¹ˆ ìƒíƒœë¡œ
                    current_words = []
                    current_text = ""
                    current_start = None
                else:
                    # ê¸€ììˆ˜/ì‹œê°„ ì´ˆê³¼ë¡œ ëŠì—ˆìœ¼ë©´ í˜„ì¬ ë‹¨ì–´ë¶€í„° ìƒˆ ë¸”ë¡
                    current_words = [word_data]
                    current_text = word
                    current_start = start
            else:
                current_words.append(word_data)
                current_text = new_text

        # ë§ˆì§€ë§‰ ë¸”ë¡
        if current_text:
            subtitles.append({
                'start': current_start,
                'end': current_words[-1].get('end', 0) if current_words else 0,
                'text': current_text
            })

        return subtitles

    # í•œêµ­ì–´ ì—°ê²°ì–´ë¯¸ (ëŠìœ¼ë©´ ì•ˆ ë˜ëŠ” íŒ¨í„´)
    # Kss ë¼ì´ë¸ŒëŸ¬ë¦¬ ê¸°ì¤€: ì´ íŒ¨í„´ìœ¼ë¡œ ëë‚˜ë©´ ë¬¸ì¥ì´ ì´ì–´ì§
    # ì£¼ì˜: ë„ˆë¬´ ì¼ë°˜ì ì¸ íŒ¨í„´('ê³ ', 'ëŠ”', 'ì€' ë“±)ì€ ì˜¤íƒ ë°©ì§€ë¥¼ ìœ„í•´ ì œì™¸
    KOREAN_CONNECTING_ENDINGS = (
        # ì›ì¸/ì´ìœ  ì—°ê²°ì–´ë¯¸ (2ê¸€ì ì´ìƒë§Œ)
        'í•´ì„œ', 'ì—ì„œ', 'ì–´ì„œ', 'ì•„ì„œ', 'ë¼ì„œ', 'ì´ë¼ì„œ',
        'ë‹ˆê¹Œ', 'ìœ¼ë‹ˆê¹Œ', 'ë‹ˆê¹', 'ìœ¼ë‹ˆê¹',
        'ë•Œë¬¸ì—', 'íƒ“ì—', 'ë•ë¶„ì—',
        # ëŒ€ì¡°/ì–‘ë³´ ì—°ê²°ì–´ë¯¸
        'ì§€ë§Œ', 'ëŠ”ë°', 'ì€ë°', 'ë”ë‹ˆ', 'ë˜ë°',
        'ëŠ”ë°ë„', 'ì€ë°ë„', 'ì§€ë§Œì€',
        # ë™ì‹œ/ë‚˜ì—´ ì—°ê²°ì–´ë¯¸ (2ê¸€ì ì´ìƒ)
        'ë©´ì„œ', 'ìœ¼ë©´ì„œ', 'ìœ¼ë©°', 'ê³ ì„œ', 'ê³ ëŠ”',
        # ëª©ì /ì˜ë„ ì—°ê²°ì–´ë¯¸
        'ë ¤ê³ ', 'ìœ¼ë ¤ê³ ', 'ìœ¼ëŸ¬', 'ë ¤ë©´', 'ìœ¼ë ¤ë©´',
        # ì¡°ê±´/ê°€ì • ì—°ê²°ì–´ë¯¸ (2ê¸€ì ì´ìƒ)
        'ìœ¼ë©´', 'ê±°ë“ ', 'ë‹¤ë©´', 'ë¼ë©´', 'ì´ë¼ë©´',
        # ì •ë„/ë¹„êµ ì—°ê²°ì–´ë¯¸
        'ë„ë¡', 'ê²Œë”', 'ë“¯ì´', 'ì²˜ëŸ¼', 'ë§Œí¼', 'ëŒ€ë¡œ',
        # ì¸ìš© ì—°ê²°ì–´ë¯¸
        'ë‹¤ê³ ', 'ë¼ê³ ', 'ëƒê³ ', 'ìê³ ',
        'ë‹¤ëŠ”', 'ë¼ëŠ”', 'ë‹¤ë‹ˆ', 'ë¼ë‹ˆ',
        # ë³´ì¡°ì  ì—°ê²°ì–´ë¯¸ (2ê¸€ì ì´ìƒ)
        'ì•„ì„œ', 'ì–´ì„œ', 'ì—¬ì„œ', 'ì•„ë„', 'ì–´ë„', 'ì—¬ë„',
        'í•˜ê²Œ', 'í•˜ì§€',  # "~í•˜ê²Œ ë˜ë‹¤", "~í•˜ì§€ ì•Šë‹¤"
    )

    def _is_korean_sentence_end(self, text: str) -> bool:
        """
        í•œêµ­ì–´ ë¬¸ì¥ ì¢…ê²° íŒ¨í„´ ì²´í¬ (Netflix/Kss ê¸°ì¤€)

        í•µì‹¬ ì›ì¹™:
        1. ë¬¸ì¥ë¶€í˜¸ â†’ ë¬´ì¡°ê±´ ëŠìŒ
        2. ì¡°ì‚¬ë¡œ ëë‚¨ â†’ ì ˆëŒ€ ëŠì§€ ì•ŠìŒ (ë¬¸ì¥ ë¶ˆì™„ì „)
        3. ì—°ê²°ì–´ë¯¸ â†’ ëŠì§€ ì•ŠìŒ (ë¬¸ì¥ì´ ì´ì–´ì§)
        4. ì¢…ê²°ì–´ë¯¸ â†’ ëŠìŒ (ë¬¸ì¥ ì™„ë£Œ)
        5. ì• ë§¤í•œ ê²½ìš° â†’ ëŠì§€ ì•ŠìŒ (ì•ˆì „í•˜ê²Œ)
        """
        if not text:
            return False

        text = text.strip()
        if not text:
            return False

        # 1. ë¬¸ì¥ë¶€í˜¸ë¡œ ëë‚˜ëŠ” ê²½ìš° - ë¬´ì¡°ê±´ ëŠìŒ
        if text[-1] in '.!?ã€‚':
            return True

        # 2. ë§ˆì§€ë§‰ ë‹¨ì–´ ì¶”ì¶œ
        words = text.split()
        if not words:
            return False

        last_word = words[-1]

        # 3. ì¡°ì‚¬ë¡œ ëë‚˜ëŠ” ê²½ìš° - ì ˆëŒ€ ëŠì§€ ì•ŠìŒ (ìµœìš°ì„ !)
        # ì˜ˆ: "ì˜¤ëŠ˜ ìš°ë¦¬ê°€" â†’ ëŠìœ¼ë©´ ì•ˆ ë¨
        # ê¸´ ì¡°ì‚¬ë¶€í„° ì²´í¬ (ì˜ˆ: "ì—ì„œ"ê°€ "ì„œ"ë³´ë‹¤ ë¨¼ì €)
        for particle in sorted(self.KOREAN_PARTICLES, key=len, reverse=True):
            if last_word.endswith(particle):
                # ë‹¨ì–´ê°€ ì¡°ì‚¬ë§Œìœ¼ë¡œ ì´ë£¨ì–´ì§„ ê²½ìš°ë„ ì²´í¬
                # ì˜ˆ: "ê°€", "ë¥¼", "ì—ì„œ" ë“±
                return False

        # 4. ì—°ê²°ì–´ë¯¸ë¡œ ëë‚˜ëŠ” ê²½ìš° - ì ˆëŒ€ ëŠì§€ ì•ŠìŒ
        # ê¸´ íŒ¨í„´ë¶€í„° ì²´í¬ (ì •í™•ë„ í–¥ìƒ)
        for connecting in sorted(self.KOREAN_CONNECTING_ENDINGS, key=len, reverse=True):
            if text.endswith(connecting):
                return False

        # 5. ì¢…ê²°ì–´ë¯¸ë¡œ ëë‚˜ëŠ” ê²½ìš° - ëŠìŒ
        # ê¸´ íŒ¨í„´ë¶€í„° ì²´í¬ (ì •í™•ë„ í–¥ìƒ)
        for ending in sorted(self.KOREAN_SENTENCE_ENDINGS, key=len, reverse=True):
            if last_word.endswith(ending):
                # ìµœì†Œ ê¸¸ì´ ê²€ì¦: ì¢…ê²°ì–´ë¯¸ë³´ë‹¤ ë‹¨ì–´ê°€ ê¸¸ì–´ì•¼ í•¨
                if len(last_word) > len(ending) or len(last_word) >= 2:
                    return True

        # 6. ì• ë§¤í•œ ê²½ìš° - ëŠì§€ ì•ŠìŒ (ì•ˆì „í•˜ê²Œ)
        return False

    def _should_break_before_word(self, word: str) -> bool:
        """
        íŠ¹ì • ë‹¨ì–´ ì•ì—ì„œ ëŠì–´ì•¼ í•˜ëŠ”ì§€ ì²´í¬ (Netflix ìŠ¤íƒ€ì¼)

        ì ‘ì†ë¶€ì‚¬ë¡œ ì‹œì‘í•˜ëŠ” ê²½ìš°:
        - "í•˜ì§€ë§Œ", "ê·¸ëŸ¬ë‚˜", "ê·¸ë˜ì„œ" ë“±ì€ ìƒˆ ë¬¸ì¥ ì‹œì‘
        - ì´ ë‹¨ì–´ë“¤ ì•ì—ì„œ ëŠì–´ì•¼ ìì—°ìŠ¤ëŸ¬ì›€
        """
        if not word:
            return False

        word = word.strip()

        # ì ‘ì†ë¶€ì‚¬ë¡œ ì‹œì‘í•˜ë©´ ì•ì—ì„œ ëŠì–´ì•¼ í•¨
        for conj in self.KOREAN_CONJUNCTIONS:
            if word.startswith(conj):
                return True

        return False

    def _split_segments_into_subtitles(self, segments: list) -> list:
        """ì„¸ê·¸ë¨¼íŠ¸ë¥¼ ì ì ˆí•œ ê¸¸ì´ë¡œ ë¶„í•  (fallback)"""
        subtitles = []

        for segment in segments:
            text = segment['text'].strip()
            start = segment['start']
            end = segment['end']
            duration = end - start

            # ì§§ì€ ì„¸ê·¸ë¨¼íŠ¸ëŠ” ê·¸ëŒ€ë¡œ
            if len(text) <= self.MAX_CHARS_PER_SUBTITLE and duration <= self.MAX_DURATION:
                subtitles.append({'start': start, 'end': end, 'text': text})
                continue

            # ê¸´ ì„¸ê·¸ë¨¼íŠ¸ëŠ” ë¶„í• 
            chunks = self._split_text_naturally(text)
            chunk_duration = duration / len(chunks) if chunks else duration

            for i, chunk in enumerate(chunks):
                chunk_start = start + (i * chunk_duration)
                chunk_end = start + ((i + 1) * chunk_duration)
                subtitles.append({
                    'start': chunk_start,
                    'end': chunk_end,
                    'text': chunk
                })

        # íƒ€ì´ë° ê²¹ì¹¨ ë³´ì •: ë‹¤ìŒ ìë§‰ ì‹œì‘ì‹œê°„ì´ ì´ì „ ìë§‰ ì¢…ë£Œì‹œê°„ë³´ë‹¤ ë¹ ë¥´ë©´ ì¡°ì •
        subtitles = self._fix_overlapping_timestamps(subtitles)

        return subtitles

    def _fix_overlapping_timestamps(self, subtitles: list) -> list:
        """
        íƒ€ì´ë° ê²¹ì¹¨ ë³´ì •: ìë§‰ ê°„ ì‹œê°„ì´ ê²¹ì¹˜ë©´ ì¡°ì •

        ì›ì¸: Whisper STTê°€ ë°˜í™˜í•˜ëŠ” segment íƒ€ì´ë°ì´ ê°€ë” ê²¹ì¹¨
        í•´ê²°: ë‹¤ìŒ ìë§‰ ì‹œì‘ì‹œê°„ = max(ë‹¤ìŒ ìë§‰ ì‹œì‘, ì´ì „ ìë§‰ ì¢…ë£Œ + gap)
        """
        if not subtitles:
            return subtitles

        MIN_GAP = 0.05  # ìµœì†Œ 50ms ê°„ê²©

        for i in range(1, len(subtitles)):
            prev_end = subtitles[i-1]['end']
            curr_start = subtitles[i]['start']

            # ê²¹ì¹¨ ë°œê²¬: ì´ì „ ìë§‰ ì¢…ë£Œ > í˜„ì¬ ìë§‰ ì‹œì‘
            if prev_end > curr_start - MIN_GAP:
                # ì´ì „ ìë§‰ ì¢…ë£Œì‹œê°„ ì§í›„ë¡œ í˜„ì¬ ìë§‰ ì‹œì‘ì‹œê°„ ì¡°ì •
                new_start = prev_end + MIN_GAP
                old_duration = subtitles[i]['end'] - subtitles[i]['start']

                subtitles[i]['start'] = new_start
                # ì¢…ë£Œ ì‹œê°„ë„ ê°™ì€ duration ìœ ì§€í•˜ë„ë¡ ì¡°ì • (ë‹¨, ë‹¤ìŒ ìë§‰ê³¼ ì•ˆ ê²¹ì¹˜ê²Œ)
                subtitles[i]['end'] = new_start + old_duration

                logger.debug(
                    f"[TimingFix] Subtitle {i}: adjusted start {curr_start:.2f}s â†’ {new_start:.2f}s"
                )

        return subtitles

    def _split_text_naturally(self, text: str) -> list:
        """í…ìŠ¤íŠ¸ë¥¼ ìì—°ìŠ¤ëŸ½ê²Œ ë¶„í•  (ë¬¸ì¥ë¶€í˜¸, ì¡°ì‚¬ ê¸°ì¤€)"""
        import re

        # ë¬¸ì¥ ë¶€í˜¸ë¡œ ë¨¼ì € ë¶„í•  (ë§ˆì¹¨í‘œ, ëŠë‚Œí‘œ, ë¬¼ìŒí‘œ)
        # í•µì‹¬: ë§ˆì¹¨í‘œ ë’¤ì—ì„œ ë¬´ì¡°ê±´ ëŠìŒ (ìµœì†Œ ê¸¸ì´ ì œí•œ ì—†ìŒ!)
        sentences = re.split(r'([.!?ã€‚])', text)
        chunks = []
        current = ""

        for i, part in enumerate(sentences):
            if not part:
                continue

            # ë¬¸ì¥ë¶€í˜¸ëŠ” ì´ì „ í…ìŠ¤íŠ¸ì— ë¶™ì„
            if part in '.!?ã€‚':
                current += part
                # ë§ˆì¹¨í‘œ ë’¤ì—ì„œ ë¬´ì¡°ê±´ ë¶„í•  (ìµœì†Œ ê¸¸ì´ ì œí•œ ì œê±°!)
                if current.strip():
                    chunks.append(current.strip())
                    current = ""
            else:
                if len(current + part) > self.MAX_CHARS_PER_SUBTITLE:
                    if current:
                        chunks.append(current.strip())
                    # ì—¬ì „íˆ ê¸´ ê²½ìš° ê°•ì œ ë¶„í•  (í•œêµ­ì–´ ë‹¨ì–´ ì¤‘ê°„ ë¶„ë¦¬ ë°©ì§€!)
                    while len(part) > self.MAX_CHARS_PER_SUBTITLE:
                        # 1ìˆœìœ„: ê³µë°± ê¸°ì¤€ ë¶„í• 
                        split_pos = part.rfind(' ', 0, self.MAX_CHARS_PER_SUBTITLE)
                        if split_pos == -1:
                            # 2ìˆœìœ„: ì¡°ì‚¬ë¡œ ëë‚˜ì§€ ì•ŠëŠ” ìœ„ì¹˜ ì°¾ê¸°
                            split_pos = self._find_safe_split_position(part, self.MAX_CHARS_PER_SUBTITLE)
                        chunks.append(part[:split_pos].strip())
                        part = part[split_pos:].strip()
                    current = part
                else:
                    current += part

        if current.strip():
            chunks.append(current.strip())

        return chunks if chunks else [text]

    def _find_safe_split_position(self, text: str, max_pos: int) -> int:
        """
        í•œêµ­ì–´ ë‹¨ì–´ ì¤‘ê°„ì—ì„œ ëŠê¸°ì§€ ì•ŠëŠ” ì•ˆì „í•œ ë¶„í•  ìœ„ì¹˜ ì°¾ê¸°

        ê·œì¹™:
        1. ì¡°ì‚¬ë¡œ ëë‚˜ëŠ” ìœ„ì¹˜ëŠ” í”¼í•¨ (ë¬¸ì¥ ë¶ˆì™„ì „)
        2. ì¢…ê²°ì–´ë¯¸ë¡œ ëë‚˜ëŠ” ìœ„ì¹˜ ìš°ì„ 
        3. ì—†ìœ¼ë©´ max_pos ë°˜í™˜ (ë¶ˆê°€í”¼í•œ ê²½ìš°)
        """
        # ë’¤ì—ì„œë¶€í„° ê²€ì‚¬ (ìµœëŒ€í•œ ê¸¸ê²Œ ìœ ì§€)
        for pos in range(min(max_pos, len(text)), max(max_pos - 6, 1), -1):
            candidate = text[:pos]

            # ì¡°ì‚¬ë¡œ ëë‚˜ë©´ SKIP (ë¬¸ì¥ ë¶ˆì™„ì „)
            is_particle = False
            for particle in self.KOREAN_PARTICLES:
                if candidate.endswith(particle) and len(candidate) > len(particle):
                    # ë‹¨, "ë•Œ", "ë°" ë“±ì€ ì¡°ì‚¬ê°€ ì•„ë‹ ìˆ˜ ìˆìŒ
                    # ê¸¸ì´ê°€ 1ì¸ ì¡°ì‚¬ë§Œ ê²€ì‚¬
                    if len(particle) == 1:
                        is_particle = True
                        break

            if not is_particle:
                # ì¢…ê²°ì–´ë¯¸ë¡œ ëë‚˜ë©´ ìµœê³ ! (ìš°ì„  ì„ íƒ)
                for ending in self.KOREAN_SENTENCE_ENDINGS:
                    if candidate.endswith(ending):
                        return pos

                # ì¢…ê²°ì–´ë¯¸ ì•„ë‹ˆë”ë¼ë„ ì¡°ì‚¬ ì•„ë‹ˆë©´ OK
                return pos

        # ì•ˆì „í•œ ìœ„ì¹˜ ëª» ì°¾ìœ¼ë©´ max_pos ë°˜í™˜
        return max_pos

    def _split_into_two_lines(self, text: str) -> str:
        """
        í…ìŠ¤íŠ¸ë¥¼ 2ì¤„ë¡œ ë¶„ë¦¬ (í•œêµ­ì–´ íŠ¹ì„± ê³ ë ¤)

        ê·œì¹™:
        1. ê° ì¤„ ìµœëŒ€ 16ì (Netflix í•œêµ­ì–´ ê¸°ì¤€)
        2. ì§§ì€ í…ìŠ¤íŠ¸(16ì ì´í•˜)ëŠ” 1ì¤„ ìœ ì§€
        3. ì¡°ì‚¬/ì–´ë¯¸ê°€ ë¶„ë¦¬ë˜ì§€ ì•Šë„ë¡ ê³µë°± ê¸°ì¤€ ë¶„í• 
        4. ê³µë°± ì—†ëŠ” í•œêµ­ì–´ëŠ” ì¤‘ê°„ì—ì„œ ë¶„ë¦¬
        """
        logger.info(f"[DEBUG 2ì¤„ ë¶„í• ] ì…ë ¥: '{text[:30]}...'")  # ğŸ”´ ë””ë²„ê·¸ ë¡œê·¸
        text = text.strip()

        # 16ì ì´í•˜ë©´ 1ì¤„ë¡œ ìœ ì§€ (ë¶ˆí•„ìš”í•œ ì¤„ë°”ê¿ˆ ë°©ì§€)
        if len(text) <= self.MAX_CHARS_PER_LINE:
            return text

        # ê³µë°±ì´ ì—†ëŠ” ê²½ìš° (ë¶™ì—¬ì“´ í•œêµ­ì–´)
        if ' ' not in text:
            # ì¤‘ê°„ì—ì„œ ë¶„ë¦¬í•˜ë˜ ìµœëŒ€ 16ì ìœ ì§€
            mid = min(len(text) // 2, self.MAX_CHARS_PER_LINE)
            line1 = text[:mid].strip()
            line2 = text[mid:].strip()

            # 2ì¤„ ëª¨ë‘ 16ì ì´ë‚´ì¸ì§€ í™•ì¸
            if len(line2) > self.MAX_CHARS_PER_LINE:
                line2 = line2[:self.MAX_CHARS_PER_LINE]

            return line1 + "\n" + line2

        # ê³µë°± ìˆëŠ” ê²½ìš° - ë¬¸ë§¥ ê¸°ë°˜ ë¶„í•  (í•œêµ­ì–´ ì¢…ê²°ì–´ë¯¸ ìš°ì„ )
        words = text.split(' ')

        if len(words) == 1:
            return text

        # ìµœì ì˜ ë¶„í•  ì§€ì  ì°¾ê¸° (ìš°ì„ ìˆœìœ„ ìˆœì„œ)
        # 1ìˆœìœ„: ì¢…ê²°ì–´ë¯¸ ë’¤ + 16ì ì´ë‚´
        # 2ìˆœìœ„: ê· ë“± ë¶„í•  + ë³´ì¡° ìš©ì–¸ íšŒí”¼
        best_split = len(words) // 2
        best_diff = float('inf')
        best_score = -1  # ë†’ì„ìˆ˜ë¡ ì¢‹ìŒ

        for i in range(1, len(words)):
            # ë³´ì¡° ìš©ì–¸ íŒ¨í„´ ê²€ì‚¬ (ë¶„ë¦¬í•˜ë©´ ì•ˆ ë˜ëŠ” ì§€ì )
            should_skip = False
            if i < len(words):
                # ë¶„í•  ì§€ì  ì•ë’¤ 2ë‹¨ì–´ í™•ì¸
                context = ' '.join(words[max(0, i-1):min(len(words), i+2)])
                for pattern in self.AUXILIARY_VERB_PATTERNS:
                    if pattern in context:
                        # íŒ¨í„´ì´ ë¶„í•  ì§€ì ì— ê±¸ì¹˜ëŠ”ì§€ í™•ì¸
                        left = ' '.join(words[:i])
                        right = ' '.join(words[i:])
                        # íŒ¨í„´ì´ ì™„ì „íˆ í•œìª½ì— ìˆìœ¼ë©´ OK, ê±¸ì¹˜ë©´ SKIP
                        if pattern in left or pattern in right:
                            pass  # ì™„ì „íˆ í•œìª½ì— ìˆìŒ - OK
                        else:
                            should_skip = True  # íŒ¨í„´ì´ ê±¸ì¹¨ - SKIP
                            break

            if should_skip:
                continue

            line1 = ' '.join(words[:i])
            line2 = ' '.join(words[i:])

            # ë‘ ì¤„ ëª¨ë‘ 16ì ì´ë‚´ì¸ ê²½ìš°ë§Œ ê³ ë ¤
            if len(line1) <= self.MAX_CHARS_PER_LINE and len(line2) <= self.MAX_CHARS_PER_LINE:
                # ì ìˆ˜ ê³„ì‚° (ë†’ì„ìˆ˜ë¡ ì¢‹ìŒ)
                score = 0

                # 1ìˆœìœ„: line2ê°€ ì¢…ê²°ì–´ë¯¸ë¡œ ëë‚˜ëŠ”ê°€? (ê°€ì¥ ì¤‘ìš”!)
                # â†’ í•œêµ­ì–´ ìì—°ìŠ¤ëŸ¬ìš´ íŒ¨í„´: ë‘ ë²ˆì§¸ ì¤„ì´ ì™„ì „í•œ ë¬¸ì¥ìœ¼ë¡œ ëë‚¨
                # ì˜ˆ: "ë§ì”€ìœ¼ë¡œ" / "ì¢‹ì€ ì•„ì¹¨ì…ë‹ˆë‹¤."
                line2_words = line2.split()
                if line2_words:
                    last_word_line2 = line2_words[-1]
                    # êµ¬ë‘ì  ì œê±° í›„ ì¢…ê²°ì–´ë¯¸ ê²€ì‚¬ (ì‰¼í‘œ, ë§ˆì¹¨í‘œ ë“±ì´ ì¢…ê²°ì–´ë¯¸ ë’¤ì— ë¶™ëŠ” ê²½ìš° ëŒ€ì‘)
                    last_word_clean = last_word_line2.rstrip(',.!?â€¦')
                    for ending in self.KOREAN_SENTENCE_ENDINGS:
                        if last_word_clean.endswith(ending) and len(last_word_clean) > len(ending):
                            logger.info(f"[DEBUG] line2 ì¢…ê²°ì–´ë¯¸ ë°œê²¬: '{line2}' (ì ìˆ˜ +100)")
                            score += 100  # ì¢…ê²°ì–´ë¯¸ ë°œê²¬ ì‹œ í° ë³´ë„ˆìŠ¤
                            break

                # 2ìˆœìœ„: ê· ë“± ë¶„í•  (ê¸€ììˆ˜ ì°¨ì´ê°€ ì ì„ìˆ˜ë¡ ì¢‹ìŒ)
                diff = abs(len(line1) - len(line2))
                score += (20 - diff)  # ì°¨ì´ê°€ 0ì´ë©´ +20, 10ì´ë©´ +10

                # ìµœê³  ì ìˆ˜ ê°±ì‹ 
                if score > best_score or (score == best_score and diff < best_diff):
                    best_score = score
                    best_diff = diff
                    best_split = i

        line1 = ' '.join(words[:best_split])
        line2 = ' '.join(words[best_split:])

        # ë‘˜ ë‹¤ ë‚´ìš©ì´ ìˆì–´ì•¼ í•¨
        if not line1 or not line2:
            mid = len(text) // 2
            return text[:mid].strip() + "\n" + text[mid:].strip()

        logger.info(f"[DEBUG ë¶„í•  ê²°ê³¼] line1: '{line1}' / line2: '{line2}'")  # ğŸ”´
        return line1 + "\n" + line2

    @staticmethod
    def _format_timestamp(seconds: float) -> str:
        """
        ì´ˆ â†’ SRT íƒ€ì„ìŠ¤íƒ¬í”„ ë³€í™˜

        Args:
            seconds: 3.5

        Returns:
            "00:00:03,500"
        """
        hours = int(seconds // 3600)
        minutes = int((seconds % 3600) // 60)
        secs = int(seconds % 60)
        millis = int((seconds % 1) * 1000)

        return f"{hours:02d}:{minutes:02d}:{secs:02d},{millis:03d}"


# ì‹±ê¸€í†¤ ì¸ìŠ¤í„´ìŠ¤
_whisper_service: WhisperService | None = None


def get_whisper_service(subtitle_length: str = "short") -> WhisperService:
    """
    WhisperService íŒ©í† ë¦¬ í•¨ìˆ˜

    Args:
        subtitle_length: ìë§‰ ê¸¸ì´ ì„¤ì • ("short" ë˜ëŠ” "long")
            - "short": 8ì/ì¤„, 16ì/ë¸”ë¡ (QT ì˜ìƒ ìµœì í™”)
            - "long": 16ì/ì¤„, 32ì/ë¸”ë¡ (Netflix í•œêµ­ì–´ ê¸°ì¤€)
    """
    # subtitle_lengthì— ë”°ë¼ ë§¤ë²ˆ ìƒˆ ì¸ìŠ¤í„´ìŠ¤ ìƒì„± (ì„¤ì •ì´ ë‹¤ë¥¼ ìˆ˜ ìˆìœ¼ë¯€ë¡œ)
    return WhisperService(subtitle_length=subtitle_length)
